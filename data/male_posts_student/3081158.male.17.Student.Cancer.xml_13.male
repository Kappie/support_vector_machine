

	 
      I've been talking to a friend of mine, and we've come to the conclusion that Africa is largely ignored in the mainstream media. We hear some stories: mostly about AIDS (but not about things we need to hear about like the extortion of pharmaceutical companies, economic policies imposed by the West to prevent people from buying cheaper, generic drugs, etc.) and certain crises that have been neglected for so long that they can no longer be ignored by mainstream media. But even then, the issues aren't addressed like they should be, like  urlLink Sudan .  Also, what does get through in the media tend to be the more profitable stories that are easily delivered and easily consumed. Africa has tons of crises with civil war and starving people being oppressed by despotic governments. Perhaps in today's world of entertainment, the countless stories of tragedy in Africa are just too hard to keep track of to the average viewer and get sucked into the memory hole?  I'd like to think it's not because the media is racist.   I know for certain that black people show up on camera. I know that's not the reason for the lack of coverage. As much as the rich elite might want to believe, Africans are not ghosts. So why does the West treat them like they don't exist? 
    
