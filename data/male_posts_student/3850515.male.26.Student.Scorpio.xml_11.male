

    
          Some methods for blindfolded record linkage  (Tim Churches  and Peter Christen)  Background  The linkage of records which refer to the same entity in separate data collections is a common requirement in public health and biomedical research. Traditionally, record linkage techniques have required that all the identifying data in which links are sought be revealed to at least one party, often a third party. This necessarily invades personal privacy and requires complete trust in the intentions of that party and their ability to maintain security and confidentiality. Dusserre, Quantin, Bouzelat and colleagues have demonstrated that it is possible to use secure one-way hash transformations to carry out follow-up epidemiological studies without any party having to reveal identifying information about any of the subjects - a technique which we refer to as "blindfolded record linkage". A limitation of their method is that only exact comparisons of values are possible, although phonetic encoding of names and other strings can be used to allow for some types of typographical variation and data errors.  Methods  A method is described which permits the calculation of a general similarity measure, the n-gram score, without having to reveal the data being compared, albeit at some cost in computation and data communication. This method can be combined with public key cryptography and automatic estimation of linkage model parameters to create an overall system for blindfolded record linkage.  Results  The system described offers good protection against misdeeds or security failures by any one party, but remains vulnerable to collusion between or simultaneous compromise of two or more parties involved in the linkage operation. In order to reduce the likelihood of this, the use of last-minute allocation of tasks to substitutable servers is proposed. Proof-of-concept computer programmes written in the Python programming language are provided to illustrate the similarity comparison protocol.  Conclusion  Although the protocols described in this paper are not unconditionally secure, they do suggest the feasibility, with the aid of modern cryptographic techniques and high speed communication networks, of a general purpose probabilistic record linkage system which permits record linkage studies to be carried out with negligible risk of invasion of personal privacy.     Resources for comparing the speed and performance of medical autocoders  (Jules J Berman)  Background  Concept indexing is a popular method for characterizing medical text, and is one of the most important early steps in many data mining efforts. Concept indexing differs from simple word or phrase indexing because concepts are typically represented by a nomenclature code that binds a medical concept to all equivalent representations. A concept search on the term renal cell carcinoma would be expected to find occurrences of hypernephroma, and renal carcinoma (concept equivalents). The purpose of this study is to provide freely available resources to compare speed and performance among different autocoders. These tools consist of: 1) a public domain autocoder written in Perl (a free and open source programming language that installs on any operating system); 2) a nomenclature database derived from the unencumbered subset of the publicly available Unified Medical Language System; 3) a large corpus of autocoded output derived from a publicly available medical text.  Methods  A simple lexical autocoder was written that parses plain-text into a listing of all 1,2,3, and 4-word strings contained in text, assigning a nomenclature code for text strings that match terms in the nomenclature. The nomenclature used is the unencumbered subset of the 2003 Unified Medical Language System (UMLS). The unencumbered subset of UMLS was reduced to exclude homonymous one-word terms and proper names, resulting in a term/code data dictionary containing about a half million medical terms. The Online Mendelian Inheritance in Man (OMIM), a 92+ Megabyte publicly available medical opus, was used as sample medical text for the autocoder.  Results  The autocoding Perl script is remarkably short, consisting of just 38 command lines. The 92+ Megabyte OMIM file was completely autocoded in 869 seconds on a 2.4 GHz processor (less than 10 seconds per Megabyte of text). The autocoded output file (9,540,442 bytes) contains 367,963 coded terms from OMIM and is distributed with this manuscript.  Conclusions  A public domain Perl script is provided that can parse through plain-text files of any length, matching concepts against an external nomenclature. The script and associated files can be used freely to compare the speed and performance of autocoding software.    A Wireless Health Outcomes Monitoring System (WHOMS): development and field testing with cancer patients using mobile phones  (Emilia Bielli, Fabio Carminati, Stella La Capra, Micaela Lina, Cinzia Brunelli  and Marcello Tamburini)  Background  Health-Related Quality of Life assessment is widely used in clinical research, but rarely in clinical practice. Barriers including practical difficulties administering printed questionnaires have limited their use. Telehealth technology could reduce these barriers and encourage better doctor-patient interaction regarding patient symptoms and quality-of-life monitoring. The aim of this study was to develop a new system for transmitting patients' self-reported outcomes using mobile phones or the internet, and to test whether patients can and will use the system via a mobile phone.  Methods  We have developed a prototype of a Wireless Health Outcomes Monitoring System, which allows structured questionnaires to be sent to the patient by their medical management team. The patients' answers are directly sent to an authorised website immediately accessible by the medical team, and are displayed in a graphic format that highlights the patient's state of health. In the present study, 97 cancer inpatients were asked to complete a ten-item questionnaire. The questionnaire was delivered by display on a mobile phone, and was answered by the patients using the mobile phone keypad.  Results  Of the 97 patients, 56 (58%) attempted the questionnaire, and all of these 56 completed it. Only 6% of the total number of questions were left unanswered by patients. Forty-one (42%) patients refused to participate, mostly due to their lack of familiarity with mobile phone use. Compared with those who completed the questionnaire, patients who refused to participate were older, had fewer years of education and were less familiar with new communications technology (mobile phone calls, mobile phone SMS, internet, email).  Conclusion  More than half of the patients self-completed the questionnaire using the mobile phone. This proportion may increase with the use of multichannel communications which can be incorporated into the system. The proportion may also increase if the patient's partner and/or family were able to assist the patient with using the technology. These preliminary results encourage further studies to identify specific diseases or circumstances where this system could be useful in patients' distance monitoring. Such a system is likely to detect patient suffering earlier, and to activate a well-timed intervention.    Description and validation of a Markov model of survival for individuals free of cardiovascular disease that uses Framingham risk factors  (Chris Martin, Mark Vanderpump,  and Joyce French)  Background  Estimation of cardiovascular disease risk is increasingly used to inform decisions on interventions, such as the use of antihypertensives and statins, or to communicate the risks of smoking. Crude 10-year cardiovascular disease risk risks may not give a realistic view of the likely impact of an intervention over a lifetime and will underestimate of the risks of smoking. A validated model of survival to act as a decision aid in the consultation may help to address these problems. This study aims to describe the development of such a model for use with people free of cardiovascular disease and evaluates its accuracy against data from a United Kingdom cohort.  Methods  A Markov cycle tree evaluated using cohort simulation was developed utilizing Framingham estimates of cardiovascular risk, 1998 United Kingdom mortality data, the relative risk for smoking related non-cardiovascular disease risk and changes in systolic blood pressure and serum total cholesterol total cholesterol with age. The model's estimates of survival at 20 years for 1391 members of the Whickham survey cohort between the ages of 35 and 65 were compared with the observed survival at 20-year follow-up.  Results  The model estimate for survival was 75% and the observed survival was 75.4%. The correlation between estimated and observed survival was 0.933 over 39 subgroups of the cohort stratified by estimated survival, 0.992 for the seven 5-year age bands from 35 to 64, 0.936 for the ten 10 mmHg systolic blood pressure bands between 100 mmHg and 200 mmHg, and 0.693 for the fifteen 0.5 mmol/l total cholesterol bands between 3.0 and 10.0 mmol/l. The model significantly underestimated mortality in those people with a systolic blood pressure greater than or equal to 180 mmHg (p = 0.006).  The average gain in life expectancy from the elimination of cardiovascular disease risk as a cause of death was 4.0 years for all the 35 year-old men in the sample (n = 24), and 1.8 years for all the 35 year-old women in the sample (n = 32).  Conclusions  This model accurately estimates 20-year survival in subjects from the Whickham cohort with a systolic blood pressure below 180 mmHg.    Estimation of hospital emergency room data using otc pharmaceutical sales and least mean square filters  (AH Najmi  and SF Magruder)  Background  Surveillance of Over-the-Counter pharmaceutical (OTC) sales as a potential early indicator of developing public health conditions, in particular in cases of interest to Bioterrorism, has been suggested in the literature. The data streams of interest are quite non-stationary and we address this problem from the viewpoint of linear adaptive filter theory: the clinical data is the primary channel which is to be estimated from the OTC data that form the reference channels.  Method  The OTC data are grouped into a few categories and we estimate the clinical data using each individual category, as well as using a multichannel filter that encompasses all the OTC categories. The estimation (in the least mean square sense) is performed using an FIR (Finite Impulse Response) filter and the normalized LMS algorithm.  Results  We show all estimation results and present a table of effectiveness of each OTC category, as well as the effectiveness of the combined filtering operation. Individual group results clearly show the effectiveness of each particular group in estimating the clinical hospital data and serve as a guide as to which groups have sustained correlations with the clinical data.  Conclusion  Our results indicate that Multichannle adaptive FIR least squares filtering is a viable means of estimating public health conditions from OTC sales, and provide quantitative measures of time dependent correlations between the clinical data and the OTC data channels.    Characterization of digital medical images utilizing support vector machines  (Ilias G Maglogiannis  and Elias P Zafiropoulos)  Background  In this paper we discuss an efficient methodology for the image analysis and characterization of digital images containing skin lesions using Support Vector Machines and present the results of a preliminary study.  Methods  The methodology is based on the support vector machines algorithm for data classification and it has been applied to the problem of the recognition of malignant melanoma versus dysplastic naevus. Border and colour based features were extracted from digital images of skin lesions acquired under reproducible conditions, using basic image processing techniques. Two alternative classification methods, the statistical discriminant analysis and the application of neural networks were also applied to the same problem and the results are compared.  Results  The SVM (Support Vector Machines) algorithm performed quite well achieving 94.1% correct classification, which is better than the performance of the other two classification methodologies. The method of discriminant analysis classified correctly 88% of cases (71% of Malignant Melanoma and 100% of Dysplastic Naevi), while the neural networks performed approximately the same.  Conclusion  The use of a computer-based system, like the one described in this paper, is intended to avoid human subjectivity and to perform specific tasks according to a number of criteria. However the presence of an expert dermatologist is considered necessary for the overall visual assessment of the skin lesion and the final diagnosis.    Monitoring of IVF birth outcomes in Finland: a data quality study  (Mika Gissler, Reija Klemetti, Tiina Sevn,  and Elina Hemminki)  Background  The collection of information on infertility treatments is important for the surveillance of potential health consequences and to monitor service provision.  Study design  We compared the coverage and outcomes of IVF children reported in aggregated IVF statistics, the Medical Birth Register (subsequently: MBR) and research data based on reimbursements for IVF treatments in Finland in 1996–1998.  Results  The number of newborns were nearly equal in the three data sources (N = 4331–4384), but the linkage between the MBR and the research data revealed that almost 40% of the reported IVF children were not the same individuals. The perinatal outcomes in the three data sources were similar, excluding the much lower incidence of major congenital anomalies in the IVF statistics (157/10 000 newborns) compared to other sources (409–422/10 000 newborns).  Conclusion  The differences in perinatal outcomes in the three data sets were in general minor, which suggests that the observed non-recording in the MBR is most likely unbiased.    Coupling computer-interpretable guidelines with a drug-database through a web-based system – The PRESGUID project  (Jean-Charles Dufour, Dominique Fieschi, and Marius Fieschi)  Background  Clinical Practice Guidelines (CPGs) available today are not extensively used due to lack of proper integration into clinical settings, knowledge-related information resources, and lack of decision support at the point of care in a particular clinical context.  Objective  The PRESGUID project (PREScription and GUIDelines) aims to improve the assistance provided by guidelines. The project proposes an online service enabling physicians to consult computerized CPGs linked to drug databases for easier integration into the healthcare process.  Methods  Computable CPGs are structured as decision trees and coded in XML format. Recommendations related to drug classes are tagged with ATC codes. We use a mapping module to enhance computerized guidelines coupling with a drug database, which contains detailed information about each usable specific medication. In this way, therapeutic recommendations are backed up with current and up-to-date information from the database.  Results  Two authoritative CPGs, originally diffused as static textual documents, have been implemented to validate the computerization process and to illustrate the usefulness of the resulting automated CPGs and their coupling with a drug database. We discuss the advantages of this approach for practitioners and the implications for both guideline developers and drug database providers. Other CPGs will be implemented and evaluated in real conditions by clinicians working in different health institutions.    Task-oriented evaluation of electronic medical records systems: development and validation of a questionnaire for physicians  (Hallvard Lrum  and Arild Faxvaag)  Background  Evaluation is a challenging but necessary part of the development cycle of clinical information systems like the electronic medical records (EMR) system. It is believed that such evaluations should include multiple perspectives, be comparative and employ both qualitative and quantitative methods. Self-administered questionnaires are frequently used as a quantitative evaluation method in medical informatics, but very few validated questionnaires address clinical use of EMR systems.  Methods  We have developed a task-oriented questionnaire for evaluating EMR systems from the clinician's perspective. The key feature of the questionnaire is a list of 24 general clinical tasks. It is applicable to physicians of most specialties and covers essential parts of their information-oriented work. The task list appears in two separate sections, about EMR use and task performance using the EMR, respectively. By combining these sections, the evaluator may estimate the potential impact of the EMR system on health care delivery. The results may also be compared across time, site or vendor. This paper describes the development, performance and validation of the questionnaire. Its performance is shown in two demonstration studies (n = 219 and 80). Its content is validated in an interview study (n = 10), and its reliability is investigated in a test-retest study (n = 37) and a scaling study (n = 31).  Results  In the interviews, the physicians found the general clinical tasks in the questionnaire relevant and comprehensible. The tasks were interpreted concordant to their definitions. However, the physicians found questions about tasks not explicitly or only partially supported by the EMR systems difficult to answer. The two demonstration studies provided unambiguous results and low percentages of missing responses. In addition, criterion validity was demonstrated for a majority of task-oriented questions. Their test-retest reliability was generally high, and the non-standard scale was found symmetric and ordinal.  Conclusion  This questionnaire is relevant for clinical work and EMR systems, provides reliable and interpretable results, and may be used as part of any evaluation effort involving the clinician's perspective of an EMR system.   
    
    
