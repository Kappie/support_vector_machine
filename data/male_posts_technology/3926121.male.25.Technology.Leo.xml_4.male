

	 
      This was going to be a post of how to tell when you or someone else is actually done with a task.  However, I am going to expand more to cover the entire process, albeit quite undetailed, for both a development and a testing scenario.  One issue that comes up is the difference between a project and a task (what is a task to one person can be a project to another person) and realizing that -done- and different percentages of -done- can be different in each case.  This is also recursive, so each checkpoint of done-ness of the project has to be -done- and therefore needs it's own checkpoints of done-ness. I suppose at a certain point, the task will be small/independent enough that it can't be considered a project.  At this point the person responsible for the task can say (and be trusted) how close the task is to being finished.  This first scenario is for a significant addition or change to an existing system.  With a little tweaking, this can probably be adapted to an entirely new system.  Below is the breakdown of the various parts of the process with percentages (of -done-) assigned to the completion of each process.  I would really like to break down each section individually and create a chart like this based on only that section (and recursive), but this will do for now.  10% - Requirements are finalized and signed off on. 20% - Development planning is done and documented, with appropriate diagrams and additional specs (or additions to the original spec).  These specs should also include boundary cases and testing cases/ideas/areas and performance requirement/expectations. 30% - Development is done (phase 1). 40% - Developer tests are done (unit tests in the 90-100% code coverage range) and signed off on by QA.  QA should sign off that the test cases are thorough, developed according to set standards and conventions, are easily runnable, and actually work. 50% - QA writes tests for the developed piece.  Depending on the size/function of what was developed, a separate functional test can be written for the piece, otherwise it can be combined into existing functional tests.  Performance tests should also be written if any performance specs were given. 60% - Development is done with incorporating QA bugs, suggestions, changes (phase 2). 70% - QA verifies phase 2 of development.  This step and the previous step should be repeated as often as needed.  QA then integrates the piece into the current acceptance, regression, load, etc. tests and/or writes new ones for the piece. 80% - Development is done incorporating QA bugs, suggestions, changes (phase 3). 90% - QA verifies and signs off on phase 3 of development.  The previous step should be repeated as often as needed. 100% - Documentation is completed by the developer, which includes javadocs, updating current documentation, and updating the original specs and requirements with the information on how the final product is different.  The project manager analyzes the project for information like how accurate the time estimates were, where the problems occurred, etc.     The second scenario is for a post-production bug fix or small change request (will be referred to as a bug for this example).  One thing to keep in mind here is that not everyone is tasked 100% for the duration of the project, however it is not always safe to task them on a different project as they may be needed back at the current project in the future.  10% - Complete description of the bug, symptoms, and how to tell it has been fixed. 20% - QA duplicates the bug and writes test that can be used to automatically duplicate the bug.  The test should be written to give the developer specific steps taken, log files, thread dumps, performance statistics, and whatever else is needed. 30% - Developer analyzes bug description, gives written analysis of what is causing the bug and what will be involved (resources, schedule estimate, possible side-effects) to fix the bug. 40% - QA analyzes the bug description and the developer's analysis, and gives written analysis on what will be involved (resources, schedule estimates) to verify the bug. 50% - Sign off by BIZ, DEV, and QA to commit to the fixing of the bug. 60% - Development planning is done and documented, with appropriate diagrams and additional specs.  If this bug fix will change something specified in a previous spec, doc, or diagram, an addendum should be made to that documentation. These specs should also include boundary cases and testing cases/ideas/areas and performance requirement/expectations. 70% - QA runs verification test, writes additional tests as needed, and incorporates change into current tests.  This step and the previous step should be repeated as often as needed. 80% - Documentation is completed by the developer, which includes javadocs, updating current documentation, and updating the original specs and requirements with the information on how the final product is different.  The project manager analyzes the project for information like how accurate the time estimates were, where the problems occurred, etc.   100% - A new build of the software is completed and released successfully.  I seem to have opened a can of worms that I don't know how to get myself out of.  I'll awkwardly end the post here, but I'll probably re-visit this topic after I have covered other topics.  writing this gives me other ideas for blogs. - What makes good requirements, differentiated by the size and function of what is being developed. - What determines well-written developer tests.  Coding conventions, non-dependence on other units, boundary cases, negative tests, etc. - Expand on the project lifecycle.  Mark tasks as BIZ, DEV, DOC, and QA.  Mark tasks as optional if needed. - Create a project checklist that can be used (and customized according to project/task). - A UML Use-Case diagram for different development processes.  Inspiration for this blog was from this article:  urlLink StickyMinds - When is Done Really Done?  
    
