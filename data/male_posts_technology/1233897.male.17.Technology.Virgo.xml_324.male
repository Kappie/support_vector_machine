

	 
      You know, there are words in geekdom, like in any other area that are taken from their original meaning and changed slightly to better facilitate release into the populous.  Words in question: Gigabyte and Hacker  LOL, I have no idea why I'm posting about this, but here goes...  First, gigabyte... having a little background on the root of the word helps to understand the problem: Computers are at the very basic, a huge, huge compilation of switches that can either be on or off, we call this base 2 (because there are only two positions that they can be in) or binary. In binary a 1 is on and a 0 is off. That's how a computer stores information, in a sequence of 1s and 0s. One space that can be occupied by either a 1 or a 0 is called a bit. A sequence of eight of these is called a byte. For reasons I won't go into here, eight just happens to be the 'magic number' of 1s and 0s that we use to communicate a standard sequence of information (like a number or letter... capital A is 01000001 in binary, lowercase a is 01100001).   The problem lies in societies' grasp of numbers, just as ancient societies used base 6 in mathematics (that's where 60 seconds in a minute, 60 minutes in an hour and other things like 360 degrees in a circle came from - all multiples of 6), so does our society use a base 10 system - all our mathematics are rooted in multiples of 10. Not so with binary, which we've already established, is base 2. Everything involving information storage in computers is figured in multiples of 2 - 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, etc. As computers advance, more and more storage space is needed bigger and bigger numbers are need to describe that larger space... First bit, then byte, which is eight bits, the a Kilobyte, which is 1024 bytes, then a Megabyte, which is 1024 Kilobytes, then Gigabyte, which is 1024 Megabytes. Now, let's stop and think about this for a minute... A Gigabyte is 8 x 1024 x 1024 x 1024 bits... that's 8,589,934,592 ones and zeros... pretty impressive huh?  Now again, back to the problem... societies' changing of a word from its orrignal meaning to something that's better understood by the populous. Our culture is based upon everything conforming to us, us sitting back and just letting it all fall into place. Case in point: tech people are starting to realize that the average non-tech person (who I call the layman)  for some reason can't figure out that a standard like Gigabyte  can  mean exaclty 1024 smaller units, not just simply 1000. Because of this, they've adopted to calling 1000 Megabytes a Gigabyte, not 1024. This is apparent in everything from gmail (check the screenshot from the last post) to labels on hard drives. Now in the case of gmail, this doesn't seem that big, superficially it's only a loss of 201326592 bits (translates to 24 Megabytes), but consider the 200 Gigabyte hard drive that one of my friends bought a few months ago... Imagine if the makers had employed this "layman's gigabyte" but instead counted  every  multiple as simply 1000 not 1024... *grabs his calculator*   Now,  8 x 1024 x 1024 x 1024 x 200 = 1717986918400 8 x 1000 x 1000 x 1000 x 200 = 1600000000000  The former subtracted by the latter is 117986918400 bits That divided by 8 is 14748364800 bytes That divided by 1024 is 14402700 kilobytes That divided by 1024 is 14065.13671875 megabyes That divided by 1024 is 13.735485076904296875 gigabytes  So what have we seen here? Societies' stuborn inability to grasp even slightly foreign concepts has cheated my friend out of   over 13 Gigabytes  .  Secondly, the word hacker... *looks at the clock* ... I'll append this post later.  
    
