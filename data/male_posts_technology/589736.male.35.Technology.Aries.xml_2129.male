

     
      On a side note about "faith"...  This past weekend, two ladies came to my door.  After greetings were passed, one promptly asked me, and I paraphrase, "Do you think you are going to Heaven when you die?".  My response was a prompt but polite "No.".  They appeared shocked and asked me why.  I said that you can't go someplace that doesn't exist.  I explained that I am an atheist, and I believe that "there is no god, or satan, or heaven, hell".  And their response was, "but you have to have faith in that, right?"  She went on to say that when you sit in a chair, do you thoroughly inspect it first, or do you just go and sit down?  Meaning that you have to have faith that the chair will support you, and won't fall apart.  My question is this....Since when did religion trademark the word "faith" as only pertaining to ones belief in a deity of some sort.  Why is it that if I have faith, then I must in some way or another believe in god?  No offense to the christians out there, but it really pissed me off.
     
    
